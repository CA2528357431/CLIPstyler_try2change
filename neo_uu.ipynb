{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from network.mynetwork_uu import Unet\n",
    "from loss.loss import CLIPLoss\n",
    "from utils.func import get_features,vgg_normalize\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = Unet(device).to(device)\n",
    "# model = Unet().to(device)\n",
    "cliploss = CLIPLoss(device)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "# vgg = torchvision.models.vgg19(pretrained=True).features.to(device)\n",
    "# for x in vgg.parameters():\n",
    "#     x.requires_grad = False\n",
    "\n",
    "topil = transforms.ToPILImage()\n",
    "topic = transforms.ToTensor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "lr1 = 0.0004\n",
    "lr2 = 0.0002\n",
    "\n",
    "lr_fast = 0.0004\n",
    "lr_slow = 0.0002\n",
    "\n",
    "dir_lambda = 500\n",
    "content_lambda = 150\n",
    "patch_lambda = 9000\n",
    "norm_lambda = 0.002\n",
    "gol_lambda = 300\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "loss_li = []\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def train(iteration1, iteration2, pic, source, target):\n",
    "    input = pic\n",
    "\n",
    "    # pic_f = get_features(vgg_normalize(pic), vgg)\n",
    "    # print(model.parameters())\n",
    "    opt = optim.Adam(model.parameters(), lr=lr1)\n",
    "    for i in range(iteration1):\n",
    "        opt.zero_grad()\n",
    "        neo_pic = model(input)\n",
    "        loss = mseloss(pic, neo_pic) * 1\n",
    "\n",
    "        # loss = 0\n",
    "        # neo_pic_f = get_features(vgg_normalize(neo_pic), vgg)\n",
    "        # loss += torch.mean((pic_f['conv4_2'] - neo_pic_f['conv4_2']) ** 2)\n",
    "        # loss += torch.mean((pic_f['conv5_2'] - neo_pic_f['conv5_2']) ** 2)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        print(\"iter:\", i + 1, \"loss:\", loss.item())\n",
    "\n",
    "        # pil = topil(neo_pic.squeeze(0).cpu())\n",
    "        # if ((i + 1) % 50) == 0:\n",
    "        #     pil.save(f\"./pic1/{(i + 1) // 50}.jpg\")\n",
    "#     neo_pic = model(input)\n",
    "#     pil = topil(neo_pic.squeeze(0).cpu())\n",
    "#     pil.save(f\"mid.jpg\")\n",
    "    # pil.save(path)\n",
    "\n",
    "\n",
    "    # torch.save(model,'unet.pth')\n",
    "\n",
    "    # model = torch.load('unet.pth')\n",
    "\n",
    "    # pic_f = get_features(vgg_normalize(pic),vgg)\n",
    "\n",
    "    opt = optim.Adam(model.parameters(), lr=lr2)\n",
    "    opt_fast = optim.Adam(model.parameters(), lr=lr2)\n",
    "    opt_slow = optim.Adam(model.parameters(), lr=lr_fast)\n",
    "    # opt_loss = optim.Adam(cliploss.parameters(), lr=lr_slow)\n",
    "    for i in range(iteration2):\n",
    "\n",
    "\n",
    "        opt.zero_grad()\n",
    "        opt_slow.zero_grad()\n",
    "        opt_fast.zero_grad()\n",
    "\n",
    "        neo_pic = model(input)\n",
    "\n",
    "        dir_loss = 0\n",
    "        dir_loss += cliploss.forward_dir(pic, source, neo_pic, target)\n",
    "\n",
    "        gol_loss = 0\n",
    "        gol_loss += cliploss.forward_gol(pic, source, neo_pic, target)\n",
    "\n",
    "        content_loss = 0\n",
    "        # content_loss += mseloss(pic, neo_pic)\n",
    "        # neo_pic_f = get_features(vgg_normalize(neo_pic), vgg)\n",
    "        # content_loss += torch.mean((pic_f['conv4_2'] - neo_pic_f['conv4_2']) ** 2)\n",
    "        # content_loss += torch.mean((pic_f['conv5_2'] - neo_pic_f['conv5_2']) ** 2)\n",
    "\n",
    "        patch_loss = 0\n",
    "#         patch_loss += cliploss.forward_patch(pic, source, neo_pic, target)\n",
    "\n",
    "        norm_loss = 0\n",
    "        norm_loss += cliploss.get_image_prior_losses(neo_pic)\n",
    "\n",
    "        loss = dir_loss * dir_lambda + \\\n",
    "               content_loss * content_lambda + \\\n",
    "               patch_loss * patch_lambda + \\\n",
    "               norm_loss * norm_lambda + \\\n",
    "               gol_loss * gol_lambda\n",
    "\n",
    "\n",
    "\n",
    "        patch_loss_fast,patch_loss_slow = cliploss.forward_patch_sec(pic, source, neo_pic, target)\n",
    "        patch_loss_fast *= patch_lambda\n",
    "        patch_loss_slow *= patch_lambda\n",
    "\n",
    "        for x in model.res2.parameters():\n",
    "            x.requires_grad = False\n",
    "        patch_loss_fast.backward(retain_graph=True)\n",
    "        for x in model.res2.parameters():\n",
    "            x.requires_grad = True\n",
    "\n",
    "        for x in model.res.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in model.conv3.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in model.upsample3.parameters():\n",
    "            x.requires_grad = False\n",
    "        patch_loss_slow.backward(retain_graph=True)\n",
    "        for x in model.res.parameters():\n",
    "            x.requires_grad = True\n",
    "        for x in model.conv3.parameters():\n",
    "            x.requires_grad = True\n",
    "        for x in model.upsample3.parameters():\n",
    "            x.requires_grad = True\n",
    "\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        opt_fast.step()\n",
    "        opt_slow.step()\n",
    "\n",
    "        loss_li.append(loss.item())\n",
    "\n",
    "        print(\"iter:\", i + 1, \"fast_loss:\", patch_loss_fast.item(), \"slow_loss:\", patch_loss_slow.item())\n",
    "        print(\"iter:\", i + 1, \"loss:\", loss.item())\n",
    "\n",
    "\n",
    "        # pil = topil(neo_pic.squeeze(0).cpu())\n",
    "        # if ((i + 1) % 10) == 0:\n",
    "        #     pil.save(f\"./pic2/{(i + 1) // 10}.jpg\")\n",
    "\n",
    "    # return  model(input)\n",
    "    # neo_pic = model(input)\n",
    "    # pil = topil(neo_pic.squeeze(0).cpu())\n",
    "    # # pil.save(f\"{source}-{target}.jpg\")\n",
    "    # pil.save(path)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "pil = Image.open(f\"./source_pic/sunflower2.jpg\")\n",
    "ori_size = pil.size[::-1]\n",
    "pil = transforms.Resize(size=(512, 512), interpolation=Image.BICUBIC)(pil)\n",
    "pic = topic(pil).unsqueeze(0).to(device)\n",
    "# pic = torch.ones(1, 3, 512, 512).to(device)\n",
    "pic.requires_grad = False\n",
    "\n",
    "source = \"photo\"\n",
    "target = \"starry night by Van Gogh\"\n",
    "# target = \"the scream by Edvard Munch\"\n",
    "# target = \"Monet\"\n",
    "# target = \"cyberpunk 2077\"\n",
    "path = \"result1.jpg\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 loss: 0.11945275962352753\n",
      "iter: 2 loss: 0.0814313292503357\n",
      "iter: 3 loss: 0.05823357403278351\n",
      "iter: 4 loss: 0.043216705322265625\n",
      "iter: 5 loss: 0.032780393958091736\n",
      "iter: 6 loss: 0.025114133954048157\n",
      "iter: 7 loss: 0.019143536686897278\n",
      "iter: 8 loss: 0.014706301502883434\n",
      "iter: 9 loss: 0.011479957029223442\n",
      "iter: 10 loss: 0.00913094263523817\n",
      "iter: 11 loss: 0.007529380731284618\n",
      "iter: 12 loss: 0.006495486944913864\n",
      "iter: 13 loss: 0.00582595681771636\n",
      "iter: 14 loss: 0.005370854400098324\n",
      "iter: 15 loss: 0.005083838477730751\n",
      "iter: 16 loss: 0.004879922606050968\n",
      "iter: 17 loss: 0.004726942162960768\n",
      "iter: 18 loss: 0.0045760804787278175\n",
      "iter: 19 loss: 0.004454623907804489\n",
      "iter: 20 loss: 0.004328466951847076\n",
      "iter: 21 loss: 0.004194471053779125\n",
      "iter: 22 loss: 0.004102181643247604\n",
      "iter: 23 loss: 0.003994944505393505\n",
      "iter: 24 loss: 0.0039053605869412422\n",
      "iter: 25 loss: 0.0038042161613702774\n",
      "iter: 26 loss: 0.003704625880345702\n",
      "iter: 27 loss: 0.0036213663406670094\n",
      "iter: 28 loss: 0.0035274382680654526\n",
      "iter: 29 loss: 0.0034505468793213367\n",
      "iter: 30 loss: 0.003357585519552231\n",
      "iter: 31 loss: 0.0032777576707303524\n",
      "iter: 32 loss: 0.003207823960110545\n",
      "iter: 33 loss: 0.003121437504887581\n",
      "iter: 34 loss: 0.003059826558455825\n",
      "iter: 35 loss: 0.0029786480590701103\n",
      "iter: 36 loss: 0.002904733642935753\n",
      "iter: 37 loss: 0.002850143238902092\n",
      "iter: 38 loss: 0.002781808143481612\n",
      "iter: 39 loss: 0.0027179457247257233\n",
      "iter: 40 loss: 0.002666973974555731\n",
      "iter: 41 loss: 0.0026078950613737106\n",
      "iter: 42 loss: 0.002550079021602869\n",
      "iter: 43 loss: 0.002498888410627842\n",
      "iter: 44 loss: 0.0024530524387955666\n",
      "iter: 45 loss: 0.0024033563677221537\n",
      "iter: 46 loss: 0.0023567872121930122\n",
      "iter: 47 loss: 0.0023138553369790316\n",
      "iter: 48 loss: 0.0022765242028981447\n",
      "iter: 49 loss: 0.00222947308793664\n",
      "iter: 50 loss: 0.0021882865112274885\n",
      "iter: 51 loss: 0.0021590949036180973\n",
      "iter: 52 loss: 0.0021232427097857\n",
      "iter: 53 loss: 0.002095649717375636\n",
      "iter: 54 loss: 0.0020572764333337545\n",
      "iter: 55 loss: 0.002032334916293621\n",
      "iter: 56 loss: 0.0019973432645201683\n",
      "iter: 57 loss: 0.001974918646737933\n",
      "iter: 58 loss: 0.0019526274409145117\n",
      "iter: 59 loss: 0.001922870404087007\n",
      "iter: 60 loss: 0.0019021776970475912\n",
      "iter: 61 loss: 0.001873247092589736\n",
      "iter: 62 loss: 0.0018562159966677427\n",
      "iter: 63 loss: 0.0018306279089301825\n",
      "iter: 64 loss: 0.0018122091423720121\n",
      "iter: 65 loss: 0.001797475153580308\n",
      "iter: 66 loss: 0.0017714833375066519\n",
      "iter: 67 loss: 0.0017511500045657158\n",
      "iter: 68 loss: 0.0017380479257553816\n",
      "iter: 69 loss: 0.0017220389563590288\n",
      "iter: 70 loss: 0.0017030106391757727\n",
      "iter: 71 loss: 0.0016870073741301894\n",
      "iter: 72 loss: 0.0016746469773352146\n",
      "iter: 73 loss: 0.0016474295407533646\n",
      "iter: 74 loss: 0.0016328028868883848\n",
      "iter: 75 loss: 0.001614521723240614\n",
      "iter: 76 loss: 0.0016008042730391026\n",
      "iter: 77 loss: 0.001576522714458406\n",
      "iter: 78 loss: 0.0015716878697276115\n",
      "iter: 79 loss: 0.00155718345195055\n",
      "iter: 80 loss: 0.001535319141112268\n",
      "iter: 81 loss: 0.001516347168944776\n",
      "iter: 82 loss: 0.0015029571950435638\n",
      "iter: 83 loss: 0.001488572801463306\n",
      "iter: 84 loss: 0.0014793795999139547\n",
      "iter: 85 loss: 0.001466530840843916\n",
      "iter: 86 loss: 0.0014460579259321094\n",
      "iter: 87 loss: 0.001434399513527751\n",
      "iter: 88 loss: 0.0014163465239107609\n",
      "iter: 89 loss: 0.0014101245906203985\n",
      "iter: 90 loss: 0.0013928720727562904\n",
      "iter: 91 loss: 0.0013832481345161796\n",
      "iter: 92 loss: 0.001363498391583562\n",
      "iter: 93 loss: 0.0013544249814003706\n",
      "iter: 94 loss: 0.0013399459421634674\n",
      "iter: 95 loss: 0.001332044368609786\n",
      "iter: 96 loss: 0.0013169748708605766\n",
      "iter: 97 loss: 0.0013011759147047997\n",
      "iter: 98 loss: 0.0012894825777038932\n",
      "iter: 99 loss: 0.0012827905593439937\n",
      "iter: 100 loss: 0.0012731421738862991\n",
      "iter: 1 fast_loss: 5048.423828125 slow_loss: 3691.6123046875\n",
      "iter: 1 loss: 810.3050537109375\n",
      "iter: 2 fast_loss: 3904.88427734375 slow_loss: 4417.876953125\n",
      "iter: 2 loss: 802.5484619140625\n",
      "iter: 3 fast_loss: 3906.9443359375 slow_loss: 4143.49365234375\n",
      "iter: 3 loss: 800.790283203125\n",
      "iter: 4 fast_loss: 3946.701171875 slow_loss: 3923.2177734375\n",
      "iter: 4 loss: 796.8081665039062\n",
      "iter: 5 fast_loss: 4073.661865234375 slow_loss: 3664.626953125\n",
      "iter: 5 loss: 781.32958984375\n",
      "iter: 6 fast_loss: 4331.35986328125 slow_loss: 3167.3583984375\n",
      "iter: 6 loss: 778.0927734375\n",
      "iter: 7 fast_loss: 3567.0546875 slow_loss: 3873.847900390625\n",
      "iter: 7 loss: 774.0936279296875\n",
      "iter: 8 fast_loss: 3377.06005859375 slow_loss: 3890.739501953125\n",
      "iter: 8 loss: 761.8604736328125\n",
      "iter: 9 fast_loss: 3786.85009765625 slow_loss: 3430.961669921875\n",
      "iter: 9 loss: 756.38427734375\n",
      "iter: 10 fast_loss: 4354.91162109375 slow_loss: 2884.87255859375\n",
      "iter: 10 loss: 753.4007568359375\n",
      "iter: 11 fast_loss: 3537.872314453125 slow_loss: 3626.44970703125\n",
      "iter: 11 loss: 756.914306640625\n",
      "iter: 12 fast_loss: 4073.318359375 slow_loss: 3059.417724609375\n",
      "iter: 12 loss: 750.9205322265625\n",
      "iter: 13 fast_loss: 3838.554443359375 slow_loss: 3300.361572265625\n",
      "iter: 13 loss: 751.188232421875\n",
      "iter: 14 fast_loss: 3703.62841796875 slow_loss: 3319.51904296875\n",
      "iter: 14 loss: 743.2083740234375\n",
      "iter: 15 fast_loss: 3424.0263671875 slow_loss: 3493.30908203125\n",
      "iter: 15 loss: 735.9811401367188\n",
      "iter: 16 fast_loss: 4108.6123046875 slow_loss: 2870.17822265625\n",
      "iter: 16 loss: 730.491943359375\n",
      "iter: 17 fast_loss: 3935.85205078125 slow_loss: 2953.46826171875\n",
      "iter: 17 loss: 738.0007934570312\n",
      "iter: 18 fast_loss: 3593.28466796875 slow_loss: 3238.357421875\n",
      "iter: 18 loss: 734.0142822265625\n",
      "iter: 19 fast_loss: 3802.57421875 slow_loss: 2984.161376953125\n",
      "iter: 19 loss: 738.2811279296875\n",
      "iter: 20 fast_loss: 3864.3037109375 slow_loss: 3063.812255859375\n",
      "iter: 20 loss: 730.5451049804688\n",
      "iter: 21 fast_loss: 3474.0830078125 slow_loss: 3372.5966796875\n",
      "iter: 21 loss: 730.0547485351562\n",
      "iter: 22 fast_loss: 2915.63427734375 slow_loss: 3888.336181640625\n",
      "iter: 22 loss: 724.8197021484375\n",
      "iter: 23 fast_loss: 3103.29443359375 slow_loss: 3607.77294921875\n",
      "iter: 23 loss: 723.8406982421875\n",
      "iter: 24 fast_loss: 3098.89990234375 slow_loss: 3699.5087890625\n",
      "iter: 24 loss: 725.8585205078125\n",
      "iter: 25 fast_loss: 3644.096435546875 slow_loss: 3119.293212890625\n",
      "iter: 25 loss: 718.12060546875\n",
      "iter: 26 fast_loss: 3312.72119140625 slow_loss: 3362.640380859375\n",
      "iter: 26 loss: 708.3741455078125\n",
      "iter: 27 fast_loss: 3636.61181640625 slow_loss: 3085.029541015625\n",
      "iter: 27 loss: 707.6380615234375\n",
      "iter: 28 fast_loss: 3382.00390625 slow_loss: 3236.02294921875\n",
      "iter: 28 loss: 706.8974609375\n",
      "iter: 29 fast_loss: 3491.59228515625 slow_loss: 3149.09375\n",
      "iter: 29 loss: 712.1595458984375\n",
      "iter: 30 fast_loss: 3279.899658203125 slow_loss: 3357.55908203125\n",
      "iter: 30 loss: 711.4242553710938\n",
      "iter: 31 fast_loss: 3196.26611328125 slow_loss: 3456.64208984375\n",
      "iter: 31 loss: 715.1840209960938\n",
      "iter: 32 fast_loss: 3382.965087890625 slow_loss: 3288.001953125\n",
      "iter: 32 loss: 711.6860961914062\n",
      "iter: 33 fast_loss: 3571.174560546875 slow_loss: 3033.46240234375\n",
      "iter: 33 loss: 708.1922607421875\n",
      "iter: 34 fast_loss: 3472.091796875 slow_loss: 3132.33935546875\n",
      "iter: 34 loss: 714.699951171875\n",
      "iter: 35 fast_loss: 4043.72412109375 slow_loss: 2490.25732421875\n",
      "iter: 35 loss: 711.46484375\n",
      "iter: 36 fast_loss: 3514.594970703125 slow_loss: 2984.779296875\n",
      "iter: 36 loss: 715.462646484375\n",
      "iter: 37 fast_loss: 3477.1728515625 slow_loss: 3122.45166015625\n",
      "iter: 37 loss: 708.2183227539062\n",
      "iter: 38 fast_loss: 3332.771240234375 slow_loss: 3163.444580078125\n",
      "iter: 38 loss: 714.4679565429688\n",
      "iter: 39 fast_loss: 3400.543212890625 slow_loss: 3080.085693359375\n",
      "iter: 39 loss: 706.9783325195312\n",
      "iter: 40 fast_loss: 3532.85986328125 slow_loss: 2994.73583984375\n",
      "iter: 40 loss: 716.245849609375\n",
      "iter: 41 fast_loss: 3322.677734375 slow_loss: 3181.77783203125\n",
      "iter: 41 loss: 703.9971923828125\n",
      "iter: 42 fast_loss: 3096.83984375 slow_loss: 3419.563232421875\n",
      "iter: 42 loss: 702.494140625\n",
      "iter: 43 fast_loss: 3297.477783203125 slow_loss: 3155.13623046875\n",
      "iter: 43 loss: 703.9989013671875\n",
      "iter: 44 fast_loss: 3098.48779296875 slow_loss: 3363.87646484375\n",
      "iter: 44 loss: 705.259521484375\n",
      "iter: 45 fast_loss: 3280.9296875 slow_loss: 3196.67822265625\n",
      "iter: 45 loss: 708.7686767578125\n",
      "iter: 46 fast_loss: 3179.85546875 slow_loss: 3259.574951171875\n",
      "iter: 46 loss: 704.5077514648438\n",
      "iter: 47 fast_loss: 3519.8134765625 slow_loss: 3004.41748046875\n",
      "iter: 47 loss: 696.0079345703125\n",
      "iter: 48 fast_loss: 2993.98046875 slow_loss: 3479.02685546875\n",
      "iter: 48 loss: 704.0113525390625\n",
      "iter: 49 fast_loss: 2897.64404296875 slow_loss: 3526.19921875\n",
      "iter: 49 loss: 701.0147705078125\n",
      "iter: 50 fast_loss: 3175.80419921875 slow_loss: 3253.669677734375\n",
      "iter: 50 loss: 704.017578125\n",
      "iter: 51 fast_loss: 3379.669189453125 slow_loss: 3029.27392578125\n",
      "iter: 51 loss: 700.2706298828125\n",
      "iter: 52 fast_loss: 3283.4013671875 slow_loss: 3142.63916015625\n",
      "iter: 52 loss: 702.0165405273438\n",
      "iter: 53 fast_loss: 3559.7763671875 slow_loss: 2860.427734375\n",
      "iter: 53 loss: 705.0155029296875\n",
      "iter: 54 fast_loss: 3286.56005859375 slow_loss: 3182.39599609375\n",
      "iter: 54 loss: 698.5152587890625\n",
      "iter: 55 fast_loss: 3472.846923828125 slow_loss: 2953.125\n",
      "iter: 55 loss: 697.7696533203125\n",
      "iter: 56 fast_loss: 3284.5 slow_loss: 3136.1845703125\n",
      "iter: 56 loss: 705.7711181640625\n",
      "iter: 57 fast_loss: 3046.440185546875 slow_loss: 3302.421630859375\n",
      "iter: 57 loss: 697.277099609375\n",
      "iter: 58 fast_loss: 3337.8525390625 slow_loss: 3051.521240234375\n",
      "iter: 58 loss: 699.5325927734375\n",
      "iter: 59 fast_loss: 3733.70361328125 slow_loss: 2640.701171875\n",
      "iter: 59 loss: 701.7877197265625\n",
      "iter: 60 fast_loss: 3486.30517578125 slow_loss: 2761.61962890625\n",
      "iter: 60 loss: 694.5330810546875\n",
      "iter: 61 fast_loss: 3160.080078125 slow_loss: 3246.391357421875\n",
      "iter: 61 loss: 700.781005859375\n",
      "iter: 62 fast_loss: 3534.370361328125 slow_loss: 2813.25537109375\n",
      "iter: 62 loss: 698.025146484375\n",
      "iter: 63 fast_loss: 3519.538818359375 slow_loss: 2796.4326171875\n",
      "iter: 63 loss: 700.0189208984375\n",
      "iter: 64 fast_loss: 3332.977294921875 slow_loss: 3004.142822265625\n",
      "iter: 64 loss: 697.0157470703125\n",
      "iter: 65 fast_loss: 3446.75439453125 slow_loss: 2928.474365234375\n",
      "iter: 65 loss: 697.5258178710938\n",
      "iter: 66 fast_loss: 3353.57666015625 slow_loss: 3016.1591796875\n",
      "iter: 66 loss: 696.2836303710938\n",
      "iter: 67 fast_loss: 3243.71337890625 slow_loss: 3105.560302734375\n",
      "iter: 67 loss: 699.029052734375\n",
      "iter: 68 fast_loss: 3000.091552734375 slow_loss: 3254.081787109375\n",
      "iter: 68 loss: 689.529541015625\n",
      "iter: 69 fast_loss: 3497.428955078125 slow_loss: 2812.7060546875\n",
      "iter: 69 loss: 699.7799072265625\n",
      "iter: 70 fast_loss: 3388.664306640625 slow_loss: 2861.869873046875\n",
      "iter: 70 loss: 694.2867431640625\n",
      "iter: 71 fast_loss: 3510.406494140625 slow_loss: 2824.310302734375\n",
      "iter: 71 loss: 697.0365600585938\n",
      "iter: 72 fast_loss: 3184.59326171875 slow_loss: 3111.740234375\n",
      "iter: 72 loss: 699.282958984375\n",
      "iter: 73 fast_loss: 3463.0966796875 slow_loss: 2776.38232421875\n",
      "iter: 73 loss: 692.5325927734375\n",
      "iter: 74 fast_loss: 3415.031494140625 slow_loss: 2909.31689453125\n",
      "iter: 74 loss: 687.034423828125\n",
      "iter: 75 fast_loss: 3383.171142578125 slow_loss: 2925.31591796875\n",
      "iter: 75 loss: 689.5322265625\n",
      "iter: 76 fast_loss: 3796.462890625 slow_loss: 2517.31103515625\n",
      "iter: 76 loss: 688.0338134765625\n",
      "iter: 77 fast_loss: 3259.849609375 slow_loss: 2976.470947265625\n",
      "iter: 77 loss: 689.5289306640625\n",
      "iter: 78 fast_loss: 3390.106201171875 slow_loss: 2871.894775390625\n",
      "iter: 78 loss: 691.52880859375\n",
      "iter: 79 fast_loss: 3283.8134765625 slow_loss: 2949.96630859375\n",
      "iter: 79 loss: 694.77783203125\n",
      "iter: 80 fast_loss: 2983.33740234375 slow_loss: 3263.28271484375\n",
      "iter: 80 loss: 687.7740478515625\n",
      "iter: 81 fast_loss: 3038.95556640625 slow_loss: 3142.707763671875\n",
      "iter: 81 loss: 692.779052734375\n",
      "iter: 82 fast_loss: 3555.03857421875 slow_loss: 2688.835205078125\n",
      "iter: 82 loss: 689.2803955078125\n",
      "iter: 83 fast_loss: 3573.028564453125 slow_loss: 2675.03369140625\n",
      "iter: 83 loss: 696.7862548828125\n",
      "iter: 84 fast_loss: 3667.304931640625 slow_loss: 2542.4423828125\n",
      "iter: 84 loss: 701.289794921875\n",
      "iter: 85 fast_loss: 3218.44482421875 slow_loss: 2973.3125\n",
      "iter: 85 loss: 692.048583984375\n",
      "iter: 86 fast_loss: 3551.74267578125 slow_loss: 2708.26708984375\n",
      "iter: 86 loss: 684.798583984375\n",
      "iter: 87 fast_loss: 3560.80615234375 slow_loss: 2651.34423828125\n",
      "iter: 87 loss: 690.294189453125\n",
      "iter: 88 fast_loss: 2948.799072265625 slow_loss: 3218.85693359375\n",
      "iter: 88 loss: 686.0387573242188\n",
      "iter: 89 fast_loss: 3409.4697265625 slow_loss: 2704.49072265625\n",
      "iter: 89 loss: 689.279296875\n",
      "iter: 90 fast_loss: 3235.542236328125 slow_loss: 2960.609375\n",
      "iter: 90 loss: 685.2738037109375\n",
      "iter: 91 fast_loss: 3211.7158203125 slow_loss: 2886.726318359375\n",
      "iter: 91 loss: 688.5201416015625\n",
      "iter: 92 fast_loss: 3159.187255859375 slow_loss: 3054.74853515625\n",
      "iter: 92 loss: 690.5230712890625\n",
      "iter: 93 fast_loss: 3506.97314453125 slow_loss: 2601.837158203125\n",
      "iter: 93 loss: 682.5321044921875\n",
      "iter: 94 fast_loss: 3511.98583984375 slow_loss: 2634.17822265625\n",
      "iter: 94 loss: 684.5323486328125\n",
      "iter: 95 fast_loss: 3019.111572265625 slow_loss: 3093.75\n",
      "iter: 95 loss: 681.5311279296875\n",
      "iter: 96 fast_loss: 3101.509033203125 slow_loss: 3010.185302734375\n",
      "iter: 96 loss: 685.7745361328125\n",
      "iter: 97 fast_loss: 3322.265625 slow_loss: 2821.906982421875\n",
      "iter: 97 loss: 685.0208740234375\n",
      "iter: 98 fast_loss: 3568.22216796875 slow_loss: 2515.1826171875\n",
      "iter: 98 loss: 680.0211791992188\n",
      "iter: 99 fast_loss: 3081.939697265625 slow_loss: 3171.61572265625\n",
      "iter: 99 loss: 692.2708129882812\n",
      "iter: 100 fast_loss: 3020.27880859375 slow_loss: 3079.536376953125\n",
      "iter: 100 loss: 683.7701416015625\n",
      "usetime: 121.35946893692017\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train(100, 100, pic, source, target)\n",
    "end = time.time()\n",
    "usetime = end - start\n",
    "print(f\"usetime: {usetime}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "neo_pic = model(pic)\n",
    "pil = topil(neo_pic.squeeze(0).cpu())\n",
    "pil = transforms.Resize(size=ori_size, interpolation=Image.BICUBIC)(pil)\n",
    "pil.save(path)\n",
    "\n",
    "\n",
    "\n",
    "# 186.4968602657318\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# with open(file = \"neo.txt\", mode = \"w\") as file:\n",
    "#     for i in loss_li:\n",
    "#         file.write(str(i)+\" \")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "#\n",
    "# x = [i for i in range(100)]\n",
    "# plt.plot(x,loss_li,color=\"red\",marker=\"o\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
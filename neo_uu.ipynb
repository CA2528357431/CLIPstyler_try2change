{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from network.mynetwork_uu import Unet\n",
    "from loss.loss import CLIPLoss\n",
    "from utils.func import get_features,vgg_normalize\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = Unet(device).to(device)\n",
    "# model = Unet().to(device)\n",
    "cliploss = CLIPLoss(device)\n",
    "mseloss = torch.nn.MSELoss()\n",
    "# vgg = torchvision.models.vgg19(pretrained=True).features.to(device)\n",
    "# for x in vgg.parameters():\n",
    "#     x.requires_grad = False\n",
    "\n",
    "topil = transforms.ToPILImage()\n",
    "topic = transforms.ToTensor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "lr1 = 0.001\n",
    "lr2 = 0.0003\n",
    "\n",
    "# lr_fast = 0.0003\n",
    "# lr_slow = 0.0004\n",
    "\n",
    "dir_lambda = 500\n",
    "content_lambda = 150\n",
    "patch_lambda = 9000\n",
    "norm_lambda = 0.002\n",
    "gol_lambda = 300\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "file_uu = open(\"neo0.txt\", \"r\")\n",
    "loss_li = file_uu.readline()\n",
    "loss_li = [float(x) for x in loss_li.split()]\n",
    "# loss_li = None\n",
    "# if not loss_li:\n",
    "#     loss_li = [0]*200\n",
    "\n",
    "# cur_times = int(file_uu.readline())\n",
    "cur_times = 0\n",
    "\n",
    "f = open(f\"./data/nums.txt\", mode=\"w\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_32032\\3157900915.py:4: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  pil = transforms.Resize(size=(512, 512), interpolation=Image.BICUBIC)(pil)\n",
      "E:\\Anaconda\\envs\\sth\\lib\\site-packages\\torchvision\\transforms\\transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pil = Image.open(f\"./source_pic/face2.jpeg\")\n",
    "# pil = Image.open(f\"resulto.jpg\")\n",
    "ori_size = pil.size[::-1]\n",
    "pil = transforms.Resize(size=(512, 512), interpolation=Image.BICUBIC)(pil)\n",
    "pic = topic(pil).unsqueeze(0).to(device)\n",
    "# pic = torch.ones(1, 3, 512, 512).to(device)\n",
    "pic.requires_grad = False\n",
    "# pic[:,1,:,:] = 0"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "source = \"photo\"\n",
    "# source = \"cat\"\n",
    "# source = \"CG picture\"\n",
    "# source = \"happy\"\n",
    "# source = \"man\"\n",
    "# source = \"cartoon\"\n",
    "# source = \"white swan\"\n",
    "\n",
    "# target = \"black swan\"\n",
    "# target = \"Mark Elliot Zuckerberg\"\n",
    "# target = \"angry\"\n",
    "# target = \"girl\"\n",
    "# target = \"Neon Light\"\n",
    "# target = \"black car\"\n",
    "# target = \"pop art\"\n",
    "# target = \"old man\"\n",
    "# target = \"Watercolor Art with Thick Brush\"\n",
    "# target = \"Pixar\"\n",
    "# target = \"watercolor painting\"\n",
    "# target = \"starry night by Van Gogh\"\n",
    "target = \"the scream by Edvard Munch\"\n",
    "# target = \"Monet\"\n",
    "# target = \"cyberpunk 2077\"\n",
    "# target = \"oil painting of flowers\"\n",
    "# target = \"man with dark black skin and black hair\"\n",
    "# target = \"The great wave off kanagawa by Hokusai\"\n",
    "# target = \"mosaic\"\n",
    "# target = \"fire\"\n",
    "# target = \"Chinese Brush Painting of mountains in black and white\"\n",
    "# target = \"Metal\"\n",
    "# target = \"sketch by Crayon\"\n",
    "# target = \"pop art of night city\"\n",
    "# target = \"a cubism style painting\"\n",
    "# target = \"Cartoon\"\n",
    "# target = \"anime\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "path = \"result/result02.jpg\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "gli = []\n",
    "dli = []\n",
    "nli = []"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train(iteration1, iteration2, pic, source, target):\n",
    "\n",
    "\n",
    "    input = pic\n",
    "\n",
    "    # pic_f = get_features(vgg_normalize(pic), vgg)\n",
    "    # print(model.parameters())\n",
    "    opt = optim.Adam(model.parameters(), lr=lr1)\n",
    "    for i in range(iteration1):\n",
    "        opt.zero_grad()\n",
    "        neo_pic = model(input)\n",
    "        loss = mseloss(pic, neo_pic) * 1\n",
    "\n",
    "        # loss = 0\n",
    "        # neo_pic_f = get_features(vgg_normalize(neo_pic), vgg)\n",
    "        # loss += torch.mean((pic_f['conv4_2'] - neo_pic_f['conv4_2']) ** 2)\n",
    "        # loss += torch.mean((pic_f['conv5_2'] - neo_pic_f['conv5_2']) ** 2)\n",
    "\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print(\"iter:\", i + 1, \"loss:\", loss.item())\n",
    "\n",
    "        # pil = topil(neo_pic.squeeze(0).cpu())\n",
    "        # if ((i + 1) % 50) == 0:\n",
    "        #     pil.save(f\"./pic1/{(i + 1) // 50}.jpg\")\n",
    "    with torch.no_grad():\n",
    "        neo_pic = model(input)\n",
    "        pil = topil(neo_pic.squeeze(0).cpu())\n",
    "        pil = transforms.Resize(size=ori_size, interpolation=Image.BICUBIC)(pil)\n",
    "        pil.save(f\"mid.jpg\")\n",
    "\n",
    "\n",
    "    # torch.save(model,'unet.pth')\n",
    "\n",
    "    # model = torch.load('unet.pth')\n",
    "\n",
    "    # pic_f = get_features(vgg_normalize(pic),vgg)\n",
    "\n",
    "    opt = optim.Adam(model.parameters(), lr=lr2)\n",
    "    # opt_fast = optim.Adam(model.parameters(), lr=lr_fast)\n",
    "    # opt_slow = optim.Adam(model.parameters(), lr=lr_slow)\n",
    "    # opt_loss = optim.Adam(cliploss.parameters(), lr=lr_slow)\n",
    "\n",
    "    for i in range(iteration2):\n",
    "\n",
    "\n",
    "        opt.zero_grad()\n",
    "        # opt_slow.zero_grad()\n",
    "        # opt_fast.zero_grad()\n",
    "\n",
    "        neo_pic = model(input)\n",
    "\n",
    "        dir_loss = 0\n",
    "        dir_loss += cliploss.forward_dir(pic, source, neo_pic, target)\n",
    "\n",
    "        gol_loss = 0\n",
    "        gol_loss += cliploss.forward_gol(pic, source, neo_pic, target)\n",
    "\n",
    "        content_loss = 0\n",
    "        # content_loss += mseloss(pic, neo_pic)\n",
    "        # neo_pic_f = get_features(vgg_normalize(neo_pic), vgg)\n",
    "        # content_loss += torch.mean((pic_f['conv4_2'] - neo_pic_f['conv4_2']) ** 2)\n",
    "        # content_loss += torch.mean((pic_f['conv5_2'] - neo_pic_f['conv5_2']) ** 2)\n",
    "\n",
    "        patch_loss = 0\n",
    "        # patch_loss += cliploss.forward_patch(pic, source, neo_pic, target)\n",
    "\n",
    "        norm_loss = 0\n",
    "        norm_loss += cliploss.forward_prior(pic, source, neo_pic, target)\n",
    "\n",
    "        loss = dir_loss * dir_lambda + \\\n",
    "               content_loss * content_lambda + \\\n",
    "               patch_loss * patch_lambda + \\\n",
    "               norm_loss * norm_lambda + \\\n",
    "               gol_loss * gol_lambda\n",
    "\n",
    "\n",
    "        patch_loss_fast,patch_loss_slow, li = cliploss.forward_patch_sec(pic, source, neo_pic, target)\n",
    "        patch_loss_fast *= patch_lambda\n",
    "        patch_loss_slow *= patch_lambda\n",
    "\n",
    "        for x in model.res2.parameters():\n",
    "            x.requires_grad = False\n",
    "        patch_loss_slow.backward(retain_graph=True)\n",
    "        for x in model.res2.parameters():\n",
    "            x.requires_grad = True\n",
    "\n",
    "        for x in model.res.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in model.conv3.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in model.upsample3.parameters():\n",
    "            x.requires_grad = False\n",
    "        for x in model.deconv3.parameters():\n",
    "            x.requires_grad = False\n",
    "        patch_loss_fast.backward(retain_graph=True)\n",
    "        for x in model.res.parameters():\n",
    "            x.requires_grad = True\n",
    "        for x in model.conv3.parameters():\n",
    "            x.requires_grad = True\n",
    "        for x in model.upsample3.parameters():\n",
    "            x.requires_grad = True\n",
    "        for x in model.deconv3.parameters():\n",
    "            x.requires_grad = True\n",
    "\n",
    "        loss.backward()\n",
    "        # (loss+patch_loss_fast+patch_loss_slow).backward()\n",
    "\n",
    "\n",
    "        opt.step()\n",
    "\n",
    "        # opt_fast.step()\n",
    "        # opt_slow.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            loss_li[i]+=(loss+patch_loss_fast+patch_loss_slow).item()\n",
    "            # loss_li[i] += (loss+patch_loss).item()\n",
    "            print(\"iter:\", i + 1, \"fast_loss:\", patch_loss_fast.item(), \"slow_loss:\", patch_loss_slow.item())\n",
    "            print(\"iter:\", i + 1, \"loss:\", (loss+patch_loss_fast+patch_loss_slow).item())\n",
    "\n",
    "\n",
    "            for x in li:\n",
    "                f.write(str(x)+\" \")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "            if (i+1)%10==0  or i==0:\n",
    "                neo_pic = model(input)\n",
    "                pil = topil(neo_pic.squeeze(0).cpu())\n",
    "                pil = transforms.Resize(size=ori_size, interpolation=Image.BICUBIC)(pil)\n",
    "                pil.save(f\"mid/{i//10}.jpg\")\n",
    "\n",
    "            gli.append(gol_loss.item()*gol_lambda)\n",
    "            dli.append(dir_loss.item()*dir_lambda)\n",
    "            nli.append(norm_loss.item()*norm_lambda)\n",
    "\n",
    "            # if (i+1)%4==0 and cliploss.patch_size>=64:\n",
    "            #     cliploss.patch_size-=8\n",
    "\n",
    "\n",
    "\n",
    "        # pil = topil(neo_pic.squeeze(0).cpu())\n",
    "        # if ((i + 1) % 10) == 0:\n",
    "        #     pil.save(f\"./pic2/{(i + 1) // 10}.jpg\")\n",
    "\n",
    "    # return  model(input)\n",
    "    # neo_pic = model(input)\n",
    "    # pil = topil(neo_pic.squeeze(0).cpu())\n",
    "    # # pil.save(f\"{source}-{target}.jpg\")\n",
    "    # pil.save(path)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 1 loss: 0.12533555924892426\n",
      "iter: 2 loss: 0.07862525433301926\n",
      "iter: 3 loss: 0.05868564173579216\n",
      "iter: 4 loss: 0.04737793654203415\n",
      "iter: 5 loss: 0.03930271044373512\n",
      "iter: 6 loss: 0.032997071743011475\n",
      "iter: 7 loss: 0.027974389493465424\n",
      "iter: 8 loss: 0.02379319816827774\n",
      "iter: 9 loss: 0.02013435587286949\n",
      "iter: 10 loss: 0.01690216176211834\n",
      "iter: 11 loss: 0.01413949579000473\n",
      "iter: 12 loss: 0.011816158890724182\n",
      "iter: 13 loss: 0.009840311482548714\n",
      "iter: 14 loss: 0.008161237463355064\n",
      "iter: 15 loss: 0.006747373845428228\n",
      "iter: 16 loss: 0.0055733658373355865\n",
      "iter: 17 loss: 0.004613175988197327\n",
      "iter: 18 loss: 0.0038399193435907364\n",
      "iter: 19 loss: 0.0032215281389653683\n",
      "iter: 20 loss: 0.0027318336069583893\n",
      "iter: 21 loss: 0.002353476593270898\n",
      "iter: 22 loss: 0.0020729980897158384\n",
      "iter: 23 loss: 0.0018763067200779915\n",
      "iter: 24 loss: 0.0017443930264562368\n",
      "iter: 25 loss: 0.0016552112065255642\n",
      "iter: 26 loss: 0.0015851492062211037\n",
      "iter: 27 loss: 0.001516756135970354\n",
      "iter: 28 loss: 0.0014416229678317904\n",
      "iter: 29 loss: 0.0013598334044218063\n",
      "iter: 30 loss: 0.0012760476674884558\n",
      "iter: 31 loss: 0.0011958959512412548\n",
      "iter: 32 loss: 0.0011227244976907969\n",
      "iter: 33 loss: 0.0010572164319455624\n",
      "iter: 34 loss: 0.0010004157666116953\n",
      "iter: 35 loss: 0.0009524842607788742\n",
      "iter: 36 loss: 0.0009129500831477344\n",
      "iter: 37 loss: 0.0008817537454888225\n",
      "iter: 38 loss: 0.0008584271417930722\n",
      "iter: 39 loss: 0.0008413562318310142\n",
      "iter: 40 loss: 0.0008281194022856653\n",
      "iter: 41 loss: 0.0008159151184372604\n",
      "iter: 42 loss: 0.000802361115347594\n",
      "iter: 43 loss: 0.0007860097102820873\n",
      "iter: 44 loss: 0.0007665226003155112\n",
      "iter: 45 loss: 0.0007446572999469936\n",
      "iter: 46 loss: 0.0007218652754090726\n",
      "iter: 47 loss: 0.0006996779120527208\n",
      "iter: 48 loss: 0.0006791411433368921\n",
      "iter: 49 loss: 0.0006604560185223818\n",
      "iter: 50 loss: 0.0006435040850192308\n",
      "iter: 51 loss: 0.0006282704416662455\n",
      "iter: 52 loss: 0.0006145281950011849\n",
      "iter: 53 loss: 0.0006017562700435519\n",
      "iter: 54 loss: 0.0005894873756915331\n",
      "iter: 55 loss: 0.0005775443278253078\n",
      "iter: 56 loss: 0.0005658261943608522\n",
      "iter: 57 loss: 0.0005541180726140738\n",
      "iter: 58 loss: 0.0005421779933385551\n",
      "iter: 59 loss: 0.0005298018222674727\n",
      "iter: 60 loss: 0.0005173079553060234\n",
      "iter: 61 loss: 0.0005054324865341187\n",
      "iter: 62 loss: 0.0004947949200868607\n",
      "iter: 63 loss: 0.0004855370498262346\n",
      "iter: 64 loss: 0.0004773550317622721\n",
      "iter: 65 loss: 0.0004699151904787868\n",
      "iter: 66 loss: 0.00046302867121994495\n",
      "iter: 67 loss: 0.0004565901472233236\n",
      "iter: 68 loss: 0.00045020546531304717\n",
      "iter: 69 loss: 0.00044347491348162293\n",
      "iter: 70 loss: 0.00043657259084284306\n",
      "iter: 71 loss: 0.0004297913983464241\n",
      "iter: 72 loss: 0.00042324778041802347\n",
      "iter: 73 loss: 0.0004168901941739023\n",
      "iter: 74 loss: 0.00041068889549933374\n",
      "iter: 75 loss: 0.0004047259863000363\n",
      "iter: 76 loss: 0.00039910440682433546\n",
      "iter: 77 loss: 0.0003937496803700924\n",
      "iter: 78 loss: 0.0003884987090714276\n",
      "iter: 79 loss: 0.0003832639195024967\n",
      "iter: 80 loss: 0.00037804077146574855\n",
      "iter: 81 loss: 0.00037277216324582696\n",
      "iter: 82 loss: 0.0003673914761748165\n",
      "iter: 83 loss: 0.00036200167960487306\n",
      "iter: 84 loss: 0.00035680155269801617\n",
      "iter: 85 loss: 0.00035185791784897447\n",
      "iter: 86 loss: 0.00034710270119830966\n",
      "iter: 87 loss: 0.0003424780152272433\n",
      "iter: 88 loss: 0.0003379635454621166\n",
      "iter: 89 loss: 0.00033354375045746565\n",
      "iter: 90 loss: 0.00032923585968092084\n",
      "iter: 91 loss: 0.00032505951821804047\n",
      "iter: 92 loss: 0.00032098504016175866\n",
      "iter: 93 loss: 0.00031700008548796177\n",
      "iter: 94 loss: 0.0003131010162178427\n",
      "iter: 95 loss: 0.0003092792467214167\n",
      "iter: 96 loss: 0.00030554510885849595\n",
      "iter: 97 loss: 0.0003019329742528498\n",
      "iter: 98 loss: 0.00029842695221304893\n",
      "iter: 99 loss: 0.00029499849188141525\n",
      "iter: 100 loss: 0.00029165076557546854\n",
      "iter: 1 fast_loss: 5126.220703125 slow_loss: 4616.04296875\n",
      "iter: 1 loss: 10553.75\n",
      "iter: 2 fast_loss: 4395.01171875 slow_loss: 4757.62939453125\n",
      "iter: 2 loss: 9940.875\n",
      "iter: 3 fast_loss: 4569.07666015625 slow_loss: 4380.11181640625\n",
      "iter: 3 loss: 9720.6728515625\n",
      "iter: 4 fast_loss: 3806.694091796875 slow_loss: 4940.27734375\n",
      "iter: 4 loss: 9506.95703125\n",
      "iter: 5 fast_loss: 4125.22900390625 slow_loss: 4462.09716796875\n",
      "iter: 5 loss: 9339.5625\n",
      "iter: 6 fast_loss: 4056.083740234375 slow_loss: 4401.947265625\n",
      "iter: 6 loss: 9202.517578125\n",
      "iter: 7 fast_loss: 3874.946533203125 slow_loss: 4454.4755859375\n",
      "iter: 7 loss: 9066.4072265625\n",
      "iter: 8 fast_loss: 3302.14697265625 slow_loss: 4875.0458984375\n",
      "iter: 8 loss: 8907.17578125\n",
      "iter: 9 fast_loss: 3379.6005859375 slow_loss: 4673.72119140625\n",
      "iter: 9 loss: 8778.052734375\n",
      "iter: 10 fast_loss: 3337.78369140625 slow_loss: 4611.99169921875\n",
      "iter: 10 loss: 8670.5048828125\n",
      "iter: 11 fast_loss: 3663.665771484375 slow_loss: 4207.90087890625\n",
      "iter: 11 loss: 8588.294921875\n",
      "iter: 12 fast_loss: 3501.68603515625 slow_loss: 4278.00732421875\n",
      "iter: 12 loss: 8493.4208984375\n",
      "iter: 13 fast_loss: 2880.683837890625 slow_loss: 4822.5859375\n",
      "iter: 13 loss: 8414.24609375\n",
      "iter: 14 fast_loss: 3895.0654296875 slow_loss: 3743.248046875\n",
      "iter: 14 loss: 8345.0390625\n",
      "iter: 15 fast_loss: 3491.11181640625 slow_loss: 4036.51416015625\n",
      "iter: 15 loss: 8228.8525390625\n",
      "iter: 16 fast_loss: 3562.31689453125 slow_loss: 3860.870361328125\n",
      "iter: 16 loss: 8118.1650390625\n",
      "iter: 17 fast_loss: 3068.4814453125 slow_loss: 4246.07861328125\n",
      "iter: 17 loss: 8004.2900390625\n",
      "iter: 18 fast_loss: 3261.29150390625 slow_loss: 3984.947265625\n",
      "iter: 18 loss: 7932.220703125\n",
      "iter: 19 fast_loss: 3129.04345703125 slow_loss: 4069.3359375\n",
      "iter: 19 loss: 7882.1142578125\n",
      "iter: 20 fast_loss: 3199.287353515625 slow_loss: 3924.5224609375\n",
      "iter: 20 loss: 7804.0478515625\n",
      "iter: 21 fast_loss: 3492.759765625 slow_loss: 3577.90380859375\n",
      "iter: 21 loss: 7748.40478515625\n",
      "iter: 22 fast_loss: 3269.462646484375 slow_loss: 3778.6103515625\n",
      "iter: 22 loss: 7724.56640625\n",
      "iter: 23 fast_loss: 3570.96875 slow_loss: 3445.17529296875\n",
      "iter: 23 loss: 7690.64013671875\n",
      "iter: 24 fast_loss: 3885.8642578125 slow_loss: 3135.84130859375\n",
      "iter: 24 loss: 7692.45263671875\n",
      "iter: 25 fast_loss: 3526.95458984375 slow_loss: 3443.595947265625\n",
      "iter: 25 loss: 7637.798828125\n",
      "iter: 26 fast_loss: 3613.4033203125 slow_loss: 3316.841064453125\n",
      "iter: 26 loss: 7594.9921875\n",
      "iter: 27 fast_loss: 3284.637451171875 slow_loss: 3627.616943359375\n",
      "iter: 27 loss: 7576.501953125\n",
      "iter: 28 fast_loss: 3176.833984375 slow_loss: 3714.68359375\n",
      "iter: 28 loss: 7555.51611328125\n",
      "iter: 29 fast_loss: 3248.931884765625 slow_loss: 3589.98876953125\n",
      "iter: 29 loss: 7502.17041015625\n",
      "iter: 30 fast_loss: 3338.7451171875 slow_loss: 3458.2900390625\n",
      "iter: 30 loss: 7456.78662109375\n",
      "iter: 31 fast_loss: 3340.18701171875 slow_loss: 3460.761962890625\n",
      "iter: 31 loss: 7458.95068359375\n",
      "iter: 32 fast_loss: 3418.18994140625 slow_loss: 3342.109619140625\n",
      "iter: 32 loss: 7413.802734375\n",
      "iter: 33 fast_loss: 3535.8125 slow_loss: 3253.39501953125\n",
      "iter: 33 loss: 7445.20947265625\n",
      "iter: 34 fast_loss: 2578.697265625 slow_loss: 4154.47998046875\n",
      "iter: 34 loss: 7384.9296875\n",
      "iter: 35 fast_loss: 3499.626220703125 slow_loss: 3236.7783203125\n",
      "iter: 35 loss: 7381.408203125\n",
      "iter: 36 fast_loss: 3177.726806640625 slow_loss: 3538.55908203125\n",
      "iter: 36 loss: 7357.291015625\n",
      "iter: 37 fast_loss: 3105.8349609375 slow_loss: 3649.4521484375\n",
      "iter: 37 loss: 7397.2919921875\n",
      "iter: 38 fast_loss: 3059.074462890625 slow_loss: 3596.855224609375\n",
      "iter: 38 loss: 7296.93408203125\n",
      "iter: 39 fast_loss: 3061.271728515625 slow_loss: 3566.230712890625\n",
      "iter: 39 loss: 7268.00634765625\n",
      "iter: 40 fast_loss: 2852.874755859375 slow_loss: 3768.85986328125\n",
      "iter: 40 loss: 7259.9873046875\n",
      "iter: 41 fast_loss: 2838.592529296875 slow_loss: 3774.55908203125\n",
      "iter: 41 loss: 7251.6552734375\n",
      "iter: 42 fast_loss: 3024.7421875 slow_loss: 3554.21435546875\n",
      "iter: 42 loss: 7214.208984375\n",
      "iter: 43 fast_loss: 3317.253173828125 slow_loss: 3234.1689453125\n",
      "iter: 43 loss: 7183.17529296875\n",
      "iter: 44 fast_loss: 3203.681884765625 slow_loss: 3336.5478515625\n",
      "iter: 44 loss: 7169.98388671875\n",
      "iter: 45 fast_loss: 3316.90966796875 slow_loss: 3227.714599609375\n",
      "iter: 45 loss: 7170.8779296875\n",
      "iter: 46 fast_loss: 3002.70068359375 slow_loss: 3531.074462890625\n",
      "iter: 46 loss: 7158.279296875\n",
      "iter: 47 fast_loss: 3318.83251953125 slow_loss: 3245.842041015625\n",
      "iter: 47 loss: 7187.6796875\n",
      "iter: 48 fast_loss: 3417.29736328125 slow_loss: 3134.53662109375\n",
      "iter: 48 loss: 7174.83984375\n",
      "iter: 49 fast_loss: 3214.25634765625 slow_loss: 3347.5341796875\n",
      "iter: 49 loss: 7184.2939453125\n",
      "iter: 50 fast_loss: 2887.619140625 slow_loss: 3619.926513671875\n",
      "iter: 50 loss: 7128.0458984375\n",
      "iter: 51 fast_loss: 3615.25732421875 slow_loss: 2973.79296875\n",
      "iter: 51 loss: 7209.04833984375\n",
      "iter: 52 fast_loss: 3607.978759765625 slow_loss: 2958.892822265625\n",
      "iter: 52 loss: 7188.619140625\n",
      "iter: 53 fast_loss: 3123.344482421875 slow_loss: 3458.7021484375\n",
      "iter: 53 loss: 7201.0439453125\n",
      "iter: 54 fast_loss: 3747.09326171875 slow_loss: 2876.97607421875\n",
      "iter: 54 loss: 7241.314453125\n",
      "iter: 55 fast_loss: 3521.46142578125 slow_loss: 3070.197998046875\n",
      "iter: 55 loss: 7209.9033203125\n",
      "iter: 56 fast_loss: 3397.1787109375 slow_loss: 3162.27734375\n",
      "iter: 56 loss: 7174.19921875\n",
      "iter: 57 fast_loss: 3110.0234375 slow_loss: 3457.3974609375\n",
      "iter: 57 loss: 7182.1640625\n",
      "iter: 58 fast_loss: 3338.264404296875 slow_loss: 3275.77978515625\n",
      "iter: 58 loss: 7230.037109375\n",
      "iter: 59 fast_loss: 3405.62451171875 slow_loss: 3167.564453125\n",
      "iter: 59 loss: 7188.43212890625\n",
      "iter: 60 fast_loss: 3385.64306640625 slow_loss: 3182.9453125\n",
      "iter: 60 loss: 7182.83203125\n",
      "iter: 61 fast_loss: 3299.262939453125 slow_loss: 3278.388916015625\n",
      "iter: 61 loss: 7194.14453125\n",
      "iter: 62 fast_loss: 4224.17431640625 slow_loss: 2391.517578125\n",
      "iter: 62 loss: 7233.6826171875\n",
      "iter: 63 fast_loss: 3829.69677734375 slow_loss: 2796.15771484375\n",
      "iter: 63 loss: 7245.34619140625\n",
      "iter: 64 fast_loss: 3141.95263671875 slow_loss: 3515.48779296875\n",
      "iter: 64 loss: 7275.68115234375\n",
      "iter: 65 fast_loss: 3876.1826171875 slow_loss: 2864.822265625\n",
      "iter: 65 loss: 7362.494140625\n",
      "iter: 66 fast_loss: 3466.461181640625 slow_loss: 3251.19775390625\n",
      "iter: 66 loss: 7338.89990234375\n",
      "iter: 67 fast_loss: 3567.329345703125 slow_loss: 3141.74658203125\n",
      "iter: 67 loss: 7331.568359375\n",
      "iter: 68 fast_loss: 3381.523193359375 slow_loss: 3343.00244140625\n",
      "iter: 68 loss: 7345.5185546875\n",
      "iter: 69 fast_loss: 3380.35595703125 slow_loss: 3319.10693359375\n",
      "iter: 69 loss: 7319.45703125\n",
      "iter: 70 fast_loss: 2849.784912109375 slow_loss: 3826.057373046875\n",
      "iter: 70 loss: 7293.3369140625\n",
      "iter: 71 fast_loss: 3271.728515625 slow_loss: 3436.86669921875\n",
      "iter: 71 loss: 7323.83837890625\n",
      "iter: 72 fast_loss: 3061.065673828125 slow_loss: 3619.1025390625\n",
      "iter: 72 loss: 7295.9111328125\n",
      "iter: 73 fast_loss: 3268.0205078125 slow_loss: 3428.48974609375\n",
      "iter: 73 loss: 7311.0029296875\n",
      "iter: 74 fast_loss: 3163.30712890625 slow_loss: 3552.909912109375\n",
      "iter: 74 loss: 7331.9599609375\n",
      "iter: 75 fast_loss: 2779.40380859375 slow_loss: 3967.71240234375\n",
      "iter: 75 loss: 7364.8583984375\n",
      "iter: 76 fast_loss: 3181.8466796875 slow_loss: 3558.33447265625\n",
      "iter: 76 loss: 7358.4228515625\n",
      "iter: 77 fast_loss: 2676.681640625 slow_loss: 4076.27099609375\n",
      "iter: 77 loss: 7368.4443359375\n",
      "iter: 78 fast_loss: 2910.346923828125 slow_loss: 3864.7841796875\n",
      "iter: 78 loss: 7387.623046875\n",
      "iter: 79 fast_loss: 2998.443603515625 slow_loss: 3751.14453125\n",
      "iter: 79 loss: 7362.080078125\n",
      "iter: 80 fast_loss: 2889.1982421875 slow_loss: 3848.85400390625\n",
      "iter: 80 loss: 7356.29345703125\n",
      "iter: 81 fast_loss: 2802.2001953125 slow_loss: 3951.576171875\n",
      "iter: 81 loss: 7370.767578125\n",
      "iter: 82 fast_loss: 2580.413818359375 slow_loss: 4152.6259765625\n",
      "iter: 82 loss: 7348.28125\n",
      "iter: 83 fast_loss: 3098.8310546875 slow_loss: 3632.148681640625\n",
      "iter: 83 loss: 7347.21875\n",
      "iter: 84 fast_loss: 3285.736083984375 slow_loss: 3417.16015625\n",
      "iter: 84 loss: 7315.38427734375\n",
      "iter: 85 fast_loss: 3812.9423828125 slow_loss: 2948.86767578125\n",
      "iter: 85 loss: 7374.29833984375\n",
      "iter: 86 fast_loss: 3899.665771484375 slow_loss: 2853.424072265625\n",
      "iter: 86 loss: 7365.3291015625\n",
      "iter: 87 fast_loss: 4129.142578125 slow_loss: 2657.867431640625\n",
      "iter: 87 loss: 7398.498046875\n",
      "iter: 88 fast_loss: 3616.0126953125 slow_loss: 3182.39599609375\n",
      "iter: 88 loss: 7411.14501953125\n",
      "iter: 89 fast_loss: 3233.82568359375 slow_loss: 3642.654296875\n",
      "iter: 89 loss: 7488.46630859375\n",
      "iter: 90 fast_loss: 3236.02294921875 slow_loss: 3646.430908203125\n",
      "iter: 90 loss: 7491.6923828125\n",
      "iter: 91 fast_loss: 3751.96826171875 slow_loss: 3126.5029296875\n",
      "iter: 91 loss: 7492.4599609375\n",
      "iter: 92 fast_loss: 3678.77197265625 slow_loss: 3243.50732421875\n",
      "iter: 92 loss: 7534.76904296875\n",
      "iter: 93 fast_loss: 3666.82421875 slow_loss: 3247.00927734375\n",
      "iter: 93 loss: 7525.0751953125\n",
      "iter: 94 fast_loss: 3674.446044921875 slow_loss: 3266.57861328125\n",
      "iter: 94 loss: 7552.26806640625\n",
      "iter: 95 fast_loss: 3554.557861328125 slow_loss: 3367.3095703125\n",
      "iter: 95 loss: 7532.61328125\n",
      "iter: 96 fast_loss: 3186.2412109375 slow_loss: 3801.750244140625\n",
      "iter: 96 loss: 7598.236328125\n",
      "iter: 97 fast_loss: 3644.92041015625 slow_loss: 3386.3984375\n",
      "iter: 97 loss: 7641.5654296875\n",
      "iter: 98 fast_loss: 2903.411865234375 slow_loss: 4127.837890625\n",
      "iter: 98 loss: 7644.744140625\n",
      "iter: 99 fast_loss: 3230.804443359375 slow_loss: 3791.999755859375\n",
      "iter: 99 loss: 7639.546875\n",
      "iter: 100 fast_loss: 3035.591064453125 slow_loss: 4023.468017578125\n",
      "iter: 100 loss: 7671.05224609375\n",
      "iter: 101 fast_loss: 3250.71728515625 slow_loss: 3822.074951171875\n",
      "iter: 101 loss: 7682.2861328125\n",
      "iter: 102 fast_loss: 3438.5146484375 slow_loss: 3591.362060546875\n",
      "iter: 102 loss: 7641.12109375\n",
      "iter: 103 fast_loss: 3432.12890625 slow_loss: 3611.96142578125\n",
      "iter: 103 loss: 7655.0849609375\n",
      "iter: 104 fast_loss: 3104.87353515625 slow_loss: 3930.976806640625\n",
      "iter: 104 loss: 7650.59375\n",
      "iter: 105 fast_loss: 3271.0419921875 slow_loss: 3871.03271484375\n",
      "iter: 105 loss: 7757.31640625\n",
      "iter: 106 fast_loss: 2827.1943359375 slow_loss: 4283.294921875\n",
      "iter: 106 loss: 7727.2314453125\n",
      "iter: 107 fast_loss: 3381.7978515625 slow_loss: 3746.20068359375\n",
      "iter: 107 loss: 7745.4912109375\n",
      "iter: 108 fast_loss: 3169.89892578125 slow_loss: 3999.504150390625\n",
      "iter: 108 loss: 7786.8955078125\n",
      "iter: 109 fast_loss: 3712.62353515625 slow_loss: 3441.535888671875\n",
      "iter: 109 loss: 7770.90234375\n",
      "iter: 110 fast_loss: 3474.76953125 slow_loss: 3652.0615234375\n",
      "iter: 110 loss: 7740.57373046875\n",
      "iter: 111 fast_loss: 3159.66796875 slow_loss: 3970.939697265625\n",
      "iter: 111 loss: 7745.8515625\n",
      "iter: 112 fast_loss: 3185.27978515625 slow_loss: 3975.0595703125\n",
      "iter: 112 loss: 7771.58544921875\n",
      "iter: 113 fast_loss: 3723.47265625 slow_loss: 3447.784423828125\n",
      "iter: 113 loss: 7781.50390625\n",
      "iter: 114 fast_loss: 4071.18994140625 slow_loss: 3145.454345703125\n",
      "iter: 114 loss: 7827.888671875\n",
      "iter: 115 fast_loss: 3829.07861328125 slow_loss: 3371.978759765625\n",
      "iter: 115 loss: 7814.05078125\n",
      "iter: 116 fast_loss: 3542.472900390625 slow_loss: 3725.8759765625\n",
      "iter: 116 loss: 7883.5947265625\n",
      "iter: 117 fast_loss: 3117.919921875 slow_loss: 4187.91943359375\n",
      "iter: 117 loss: 7920.58251953125\n",
      "iter: 118 fast_loss: 3560.943603515625 slow_loss: 3749.015869140625\n",
      "iter: 118 loss: 7925.94921875\n",
      "iter: 119 fast_loss: 3433.708251953125 slow_loss: 3880.302490234375\n",
      "iter: 119 loss: 7930.25\n",
      "iter: 120 fast_loss: 3679.115234375 slow_loss: 3677.87939453125\n",
      "iter: 120 loss: 7971.2353515625\n",
      "iter: 121 fast_loss: 3936.74462890625 slow_loss: 3483.283935546875\n",
      "iter: 121 loss: 8034.7705078125\n",
      "iter: 122 fast_loss: 3476.41748046875 slow_loss: 3911.064208984375\n",
      "iter: 122 loss: 8001.4765625\n",
      "iter: 123 fast_loss: 3717.292724609375 slow_loss: 3688.041748046875\n",
      "iter: 123 loss: 8018.58203125\n",
      "iter: 124 fast_loss: 3482.8720703125 slow_loss: 3927.886962890625\n",
      "iter: 124 loss: 8025.5048828125\n",
      "iter: 125 fast_loss: 3424.369873046875 slow_loss: 4078.46826171875\n",
      "iter: 125 loss: 8117.58349609375\n",
      "iter: 126 fast_loss: 3515.4189453125 slow_loss: 3968.742431640625\n",
      "iter: 126 loss: 8096.908203125\n",
      "iter: 127 fast_loss: 3537.872314453125 slow_loss: 3961.738525390625\n",
      "iter: 127 loss: 8113.8583984375\n",
      "iter: 128 fast_loss: 3189.3310546875 slow_loss: 4289.0625\n",
      "iter: 128 loss: 8094.14453125\n",
      "iter: 129 fast_loss: 3615.531982421875 slow_loss: 3843.97900390625\n",
      "iter: 129 loss: 8078.76123046875\n",
      "iter: 130 fast_loss: 3764.739990234375 slow_loss: 3735.420166015625\n",
      "iter: 130 loss: 8118.6611328125\n",
      "iter: 131 fast_loss: 3650.688232421875 slow_loss: 3861.762939453125\n",
      "iter: 131 loss: 8129.9521484375\n",
      "iter: 132 fast_loss: 3429.0390625 slow_loss: 4081.76416015625\n",
      "iter: 132 loss: 8130.8046875\n",
      "iter: 133 fast_loss: 3694.770751953125 slow_loss: 3876.045166015625\n",
      "iter: 133 loss: 8189.81640625\n",
      "iter: 134 fast_loss: 3808.20458984375 slow_loss: 3780.94482421875\n",
      "iter: 134 loss: 8208.6484375\n",
      "iter: 135 fast_loss: 3681.65576171875 slow_loss: 3913.055419921875\n",
      "iter: 135 loss: 8213.7119140625\n",
      "iter: 136 fast_loss: 3259.09423828125 slow_loss: 4395.35546875\n",
      "iter: 136 loss: 8271.703125\n",
      "iter: 137 fast_loss: 3139.75537109375 slow_loss: 4520.11865234375\n",
      "iter: 137 loss: 8273.62890625\n",
      "iter: 138 fast_loss: 3150.32958984375 slow_loss: 4529.044921875\n",
      "iter: 138 loss: 8295.1318359375\n",
      "iter: 139 fast_loss: 3298.3017578125 slow_loss: 4423.16455078125\n",
      "iter: 139 loss: 8337.4736328125\n",
      "iter: 140 fast_loss: 3548.99609375 slow_loss: 4209.9609375\n",
      "iter: 140 loss: 8374.4638671875\n",
      "iter: 141 fast_loss: 3543.708740234375 slow_loss: 4271.00390625\n",
      "iter: 141 loss: 8431.7197265625\n",
      "iter: 142 fast_loss: 3552.497802734375 slow_loss: 4281.578125\n",
      "iter: 142 loss: 8452.583984375\n",
      "iter: 143 fast_loss: 3583.328125 slow_loss: 4323.326171875\n",
      "iter: 143 loss: 8524.4111328125\n",
      "iter: 144 fast_loss: 3099.58642578125 slow_loss: 4789.626953125\n",
      "iter: 144 loss: 8506.216796875\n",
      "iter: 145 fast_loss: 3700.05810546875 slow_loss: 4204.39892578125\n",
      "iter: 145 loss: 8519.70703125\n",
      "iter: 146 fast_loss: 3350.349365234375 slow_loss: 4551.77294921875\n",
      "iter: 146 loss: 8515.8701171875\n",
      "iter: 147 fast_loss: 4097.6943359375 slow_loss: 3846.038818359375\n",
      "iter: 147 loss: 8556.4814453125\n",
      "iter: 148 fast_loss: 4182.220703125 slow_loss: 3792.6865234375\n",
      "iter: 148 loss: 8587.65625\n",
      "iter: 149 fast_loss: 3622.87890625 slow_loss: 4420.8984375\n",
      "iter: 149 loss: 8656.02734375\n",
      "iter: 150 fast_loss: 3535.6064453125 slow_loss: 4541.748046875\n",
      "iter: 150 loss: 8691.107421875\n",
      "iter: 151 fast_loss: 3431.37353515625 slow_loss: 4711.1435546875\n",
      "iter: 151 loss: 8755.771484375\n",
      "iter: 152 fast_loss: 3815.9638671875 slow_loss: 4376.7470703125\n",
      "iter: 152 loss: 8805.46484375\n",
      "iter: 153 fast_loss: 3715.095458984375 slow_loss: 4504.25732421875\n",
      "iter: 153 loss: 8828.10546875\n",
      "iter: 154 fast_loss: 3615.25732421875 slow_loss: 4646.04931640625\n",
      "iter: 154 loss: 8867.806640625\n",
      "iter: 155 fast_loss: 3723.12939453125 slow_loss: 4539.138671875\n",
      "iter: 155 loss: 8867.2646484375\n",
      "iter: 156 fast_loss: 3478.68359375 slow_loss: 4779.6708984375\n",
      "iter: 156 loss: 8864.3515625\n",
      "iter: 157 fast_loss: 3389.213623046875 slow_loss: 4898.25439453125\n",
      "iter: 157 loss: 8897.71875\n",
      "iter: 158 fast_loss: 3041.290283203125 slow_loss: 5265.953125\n",
      "iter: 158 loss: 8913.244140625\n",
      "iter: 159 fast_loss: 3280.448974609375 slow_loss: 5001.5947265625\n",
      "iter: 159 loss: 8891.5458984375\n",
      "iter: 160 fast_loss: 3116.889892578125 slow_loss: 5129.58544921875\n",
      "iter: 160 loss: 8852.9736328125\n",
      "iter: 161 fast_loss: 3011.7646484375 slow_loss: 5252.357421875\n",
      "iter: 161 loss: 8876.37109375\n",
      "iter: 162 fast_loss: 3681.86181640625 slow_loss: 4610.9619140625\n",
      "iter: 162 loss: 8916.3251953125\n",
      "iter: 163 fast_loss: 3420.9365234375 slow_loss: 4855.544921875\n",
      "iter: 163 loss: 8893.4833984375\n",
      "iter: 164 fast_loss: 4187.1640625 slow_loss: 4144.7294921875\n",
      "iter: 164 loss: 8950.64453125\n",
      "iter: 165 fast_loss: 3561.21826171875 slow_loss: 4769.30224609375\n",
      "iter: 165 loss: 8951.521484375\n",
      "iter: 166 fast_loss: 3289.924560546875 slow_loss: 5009.21630859375\n",
      "iter: 166 loss: 8921.140625\n",
      "iter: 167 fast_loss: 3688.65966796875 slow_loss: 4633.48388671875\n",
      "iter: 167 loss: 8941.640625\n",
      "iter: 168 fast_loss: 3550.23193359375 slow_loss: 4783.03515625\n",
      "iter: 168 loss: 8953.265625\n",
      "iter: 169 fast_loss: 4488.94482421875 slow_loss: 3916.694580078125\n",
      "iter: 169 loss: 9027.642578125\n",
      "iter: 170 fast_loss: 3467.765869140625 slow_loss: 4948.72265625\n",
      "iter: 170 loss: 9042.99609375\n",
      "iter: 171 fast_loss: 3328.788818359375 slow_loss: 5047.05029296875\n",
      "iter: 171 loss: 9001.095703125\n",
      "iter: 172 fast_loss: 4117.33251953125 slow_loss: 4281.990234375\n",
      "iter: 172 loss: 9022.32421875\n",
      "iter: 173 fast_loss: 3433.158935546875 slow_loss: 4892.6923828125\n",
      "iter: 173 loss: 8949.3486328125\n",
      "iter: 174 fast_loss: 3981.719970703125 slow_loss: 4400.642578125\n",
      "iter: 174 loss: 9005.859375\n",
      "iter: 175 fast_loss: 3849.8154296875 slow_loss: 4536.873046875\n",
      "iter: 175 loss: 9009.6865234375\n",
      "iter: 176 fast_loss: 3853.66064453125 slow_loss: 4529.6630859375\n",
      "iter: 176 loss: 9008.076171875\n",
      "iter: 177 fast_loss: 4247.86376953125 slow_loss: 4173.84326171875\n",
      "iter: 177 loss: 9049.21484375\n",
      "iter: 178 fast_loss: 3460.55615234375 slow_loss: 4916.24462890625\n",
      "iter: 178 loss: 9004.314453125\n",
      "iter: 179 fast_loss: 4235.50390625 slow_loss: 4156.333984375\n",
      "iter: 179 loss: 9020.60546875\n",
      "iter: 180 fast_loss: 3465.7744140625 slow_loss: 4953.94140625\n",
      "iter: 180 loss: 9052.484375\n",
      "iter: 181 fast_loss: 4262.4892578125 slow_loss: 4208.99951171875\n",
      "iter: 181 loss: 9104.7578125\n",
      "iter: 182 fast_loss: 3996.68896484375 slow_loss: 4461.5478515625\n",
      "iter: 182 loss: 9092.50390625\n",
      "iter: 183 fast_loss: 4414.44384765625 slow_loss: 4081.146240234375\n",
      "iter: 183 loss: 9135.60546875\n",
      "iter: 184 fast_loss: 4305.5419921875 slow_loss: 4237.35791015625\n",
      "iter: 184 loss: 9182.412109375\n",
      "iter: 185 fast_loss: 3661.39990234375 slow_loss: 4848.47265625\n",
      "iter: 185 loss: 9149.3828125\n",
      "iter: 186 fast_loss: 4185.1728515625 slow_loss: 4353.263671875\n",
      "iter: 186 loss: 9181.9453125\n",
      "iter: 187 fast_loss: 4290.22998046875 slow_loss: 4224.6552734375\n",
      "iter: 187 loss: 9164.642578125\n",
      "iter: 188 fast_loss: 3607.635498046875 slow_loss: 4859.6650390625\n",
      "iter: 188 loss: 9123.3076171875\n",
      "iter: 189 fast_loss: 4276.359375 slow_loss: 4217.30810546875\n",
      "iter: 189 loss: 9152.42578125\n",
      "iter: 190 fast_loss: 4068.305908203125 slow_loss: 4465.1181640625\n",
      "iter: 190 loss: 9194.1826171875\n",
      "iter: 191 fast_loss: 3427.0478515625 slow_loss: 5142.90625\n",
      "iter: 191 loss: 9230.96484375\n",
      "iter: 192 fast_loss: 4459.69384765625 slow_loss: 4127.90673828125\n",
      "iter: 192 loss: 9249.1123046875\n",
      "iter: 193 fast_loss: 3693.26025390625 slow_loss: 4890.4267578125\n",
      "iter: 193 loss: 9247.201171875\n",
      "iter: 194 fast_loss: 3649.31494140625 slow_loss: 4875.732421875\n",
      "iter: 194 loss: 9188.564453125\n",
      "iter: 195 fast_loss: 3564.445556640625 slow_loss: 4997.0625\n",
      "iter: 195 loss: 9220.52734375\n",
      "iter: 196 fast_loss: 4485.443359375 slow_loss: 4115.7529296875\n",
      "iter: 196 loss: 9257.96875\n",
      "iter: 197 fast_loss: 3829.216064453125 slow_loss: 4782.00537109375\n",
      "iter: 197 loss: 9267.74609375\n",
      "iter: 198 fast_loss: 3577.423095703125 slow_loss: 5034.69091796875\n",
      "iter: 198 loss: 9266.888671875\n",
      "iter: 199 fast_loss: 4653.1220703125 slow_loss: 4024.566650390625\n",
      "iter: 199 loss: 9333.21484375\n",
      "iter: 200 fast_loss: 4527.60302734375 slow_loss: 4171.302734375\n",
      "iter: 200 loss: 9352.9345703125\n",
      "usetime: 149.69095015525818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_32032\\4187812.py:31: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  pil = transforms.Resize(size=ori_size, interpolation=Image.BICUBIC)(pil)\n",
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_32032\\4187812.py:132: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  pil = transforms.Resize(size=ori_size, interpolation=Image.BICUBIC)(pil)\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "train(100, 100, pic, source, target)\n",
    "end = time.time()\n",
    "usetime = end - start\n",
    "print(f\"usetime: {usetime}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\win10\\AppData\\Local\\Temp\\ipykernel_32032\\4189447853.py:3: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  pil = transforms.Resize(size=ori_size, interpolation=Image.BICUBIC)(pil)\n"
     ]
    }
   ],
   "source": [
    "neo_pic = model(pic)\n",
    "pil = topil(neo_pic.squeeze(0).cpu())\n",
    "pil = transforms.Resize(size=ori_size, interpolation=Image.BICUBIC)(pil)\n",
    "pil.save(path)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# with open(file = \"neo0.txt\", mode = \"w\") as file:\n",
    "#     for i in loss_li:\n",
    "#         file.write(str(i)+\" \")\n",
    "#     file.write(\"\\n\")\n",
    "#     file.write(str(cur_times+1))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (200,)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [14], line 5\u001B[0m\n\u001B[0;32m      3\u001B[0m x \u001B[38;5;241m=\u001B[39m [i \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m100\u001B[39m)]\n\u001B[0;32m      4\u001B[0m loss_li \u001B[38;5;241m=\u001B[39m [x\u001B[38;5;241m/\u001B[39m(cur_times\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m loss_li]\n\u001B[1;32m----> 5\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43mloss_li\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcolor\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mred\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mmarker\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mo\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Anaconda\\envs\\sth\\lib\\site-packages\\matplotlib\\pyplot.py:2767\u001B[0m, in \u001B[0;36mplot\u001B[1;34m(scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2765\u001B[0m \u001B[38;5;129m@_copy_docstring_and_deprecators\u001B[39m(Axes\u001B[38;5;241m.\u001B[39mplot)\n\u001B[0;32m   2766\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot\u001B[39m(\u001B[38;5;241m*\u001B[39margs, scalex\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, scaley\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m-> 2767\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgca\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2768\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscalex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscalex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscaley\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mscaley\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2769\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m}\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Anaconda\\envs\\sth\\lib\\site-packages\\matplotlib\\axes\\_axes.py:1635\u001B[0m, in \u001B[0;36mAxes.plot\u001B[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1393\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1394\u001B[0m \u001B[38;5;124;03mPlot y versus x as lines and/or markers.\u001B[39;00m\n\u001B[0;32m   1395\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1632\u001B[0m \u001B[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001B[39;00m\n\u001B[0;32m   1633\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1634\u001B[0m kwargs \u001B[38;5;241m=\u001B[39m cbook\u001B[38;5;241m.\u001B[39mnormalize_kwargs(kwargs, mlines\u001B[38;5;241m.\u001B[39mLine2D)\n\u001B[1;32m-> 1635\u001B[0m lines \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_lines(\u001B[38;5;241m*\u001B[39margs, data\u001B[38;5;241m=\u001B[39mdata, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)]\n\u001B[0;32m   1636\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m line \u001B[38;5;129;01min\u001B[39;00m lines:\n\u001B[0;32m   1637\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39madd_line(line)\n",
      "File \u001B[1;32mE:\\Anaconda\\envs\\sth\\lib\\site-packages\\matplotlib\\axes\\_base.py:312\u001B[0m, in \u001B[0;36m_process_plot_var_args.__call__\u001B[1;34m(self, data, *args, **kwargs)\u001B[0m\n\u001B[0;32m    310\u001B[0m     this \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    311\u001B[0m     args \u001B[38;5;241m=\u001B[39m args[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m--> 312\u001B[0m \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_plot_args\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Anaconda\\envs\\sth\\lib\\site-packages\\matplotlib\\axes\\_base.py:498\u001B[0m, in \u001B[0;36m_process_plot_var_args._plot_args\u001B[1;34m(self, tup, kwargs, return_kwargs)\u001B[0m\n\u001B[0;32m    495\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maxes\u001B[38;5;241m.\u001B[39myaxis\u001B[38;5;241m.\u001B[39mupdate_units(y)\n\u001B[0;32m    497\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m!=\u001B[39m y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m]:\n\u001B[1;32m--> 498\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y must have same first dimension, but \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    499\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhave shapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    500\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m x\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m y\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m    501\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mx and y can be no greater than 2D, but have \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    502\u001B[0m                      \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mshapes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mx\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00my\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: x and y must have same first dimension, but have shapes (100,) and (200,)"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [i for i in range(100)]\n",
    "loss_li = [x/(cur_times+1) for x in loss_li]\n",
    "plt.plot(x,loss_li,color=\"red\",marker=\"o\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = [i for i in range(100)]\n",
    "plt.plot(x,gli,color=\"red\",marker=\"o\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = [i for i in range(100)]\n",
    "plt.plot(x,dli,color=\"red\",marker=\"o\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = [i for i in range(100)]\n",
    "plt.plot(x,nli,color=\"red\",marker=\"o\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = [i for i in range(100)]\n",
    "li = [gli[i]+dli[i]+nli[i] for i in range(100)]\n",
    "plt.plot(x,li,color=\"red\",marker=\"o\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"accurate rate: {cliploss.right_patch/6400}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}